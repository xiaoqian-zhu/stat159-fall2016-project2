---
title: "Predictive Modeling Process"
author: "Shirley Jin, Xiaoqian Zhu"
data: "November 4, 2016"
output: pdf_document
---
```{r, echo = FALSE, include = FALSE, eval=TRUE}
# load the necessary data files and required packages
scaled_credit <- read.csv("../data/datasets/scaled-credit.csv")
load("../data/output/ols-regression.RData")
load("../data/output/lasso-regression.RData")
load("../data/output/ridge-regression.RData")
load("../data/output/pc-regression.RData")
load("../data/output/pls-regression.RData")

# packages
library(xtable)
library(png)
library(grid)
library(ggplot2)
library(reshape)
```

#Abstract


In this project, we are going to take a look at five different regression models, including, ordinary least squares, ridge, lasso, principal components, and partial least squares regressions. We are mainly following the analysis from _Linear Model Selection and Regularization_(chapter 6) of the *An Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. [Link to the book](http://www-bcf.usc.edu/~gareth/ISL/). Apply each model on the data set _Credit_, [Link to the data set](http://www-bcf.usc.edu/~gareth/ISL/Credit.csv), we predict the variable `Balance ` in terms of ten predictors such as `Income`, `Age`, `Education`, `Gender`, `Ethnicity`, etc and also describe the differences between these models.
# Introduction


In this project we are going to follow the analysis from _Linear Model Selection and Regularization_(chapter 6) of the *An Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. [Link to the book](http://www-bcf.usc.edu/~gareth/ISL/). Applying each model on the data set _Credit_, [Link to the data set](http://www-bcf.usc.edu/~gareth/ISL/Credit.csv), we are going to predict the variable `Balance ` in terms of ten predictors such as `Income`, `Age`, `Education`, `Gender`, `Ethnicity`, etc and also describe the differences between these models.Addtionally, this project involves working collaboratively in teams of two members.

In previous homework, we used simple and multiple linear regression for data analysis, while in this project, we consider some approaches for improving the simple linear model, by replacing plain least squares fitting with some alternative fitting procedures. _So, why might we want to use another fitting procedure instead of least squares?_

The linear model assumes that the true relationship between the response and the predictors is approximately linear and then, the least squares estimates will have low bias. If the number of observations is larger than the number of variables, the least squares estimates tend to have low variace and hence will perform well on test observations. However, if number of observations is not larger than the number of variables, then there can be a lot of variabiblity in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. If the number of observations is smaller than the number of variables, then the method could not be used. This results in failures in _prediction accuracy_. 

Moreover, It is often the case that some or many of the
variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero.This results in failures in _Model Interpretability_.

Using alterantive fitting procedures can yield better prediction accuracy and model interpretability, by constraining or shrinking the estimated coefficients and performing feature selection or variable selection, and also reducing dimension. 

In this report, we are going to focus shrinkage methods and dimension reduction method. Ridge and Lasso regressions are shrinkage methods. PCR and PLSR are dimension reduction methods. Functions to fit models with ridge and lasso regressions are available in the package `"glmnet"`. Functions to fit models with PCR and PLSR are available in the package `"pls"`. We use the multiple linear regression model via Ordinary Least Squares(OLS) based on the data set `credit` as the benchmark to compare the other methods. 


# Data

In this project, we are using the data set `Credit.csv`, which includes some qualitative and quantitative variables. The quantitative varaibels includes `age`, `cards`, `balance`, `income`, `limit`, `education`, `rating`, which represent the sample's age, the number of credit cards, years of education, income measured in thousands of dollars, credit limit, credit rating and the sample's average credit card debt. The qualitative variables are `gender`, `student`, `status` and `ethnicity`, which represent whether the sample is a student, is married, and also the specific ethnicity.

However, before we can fit any model, we have to perform two major processing steps on the data set:

- convert factors into dummy variables
- mean centering and standardization

The first step involves transforming each categorical variable 
(`Gender`, `Student`, `Married`, and `Ethnicity`) into dummy variables. The main reason to do this is because the function `glmnet()` (used in ridge andlasso regressions) will not work if the input data contains factors.

The second processing step involves mean-centering and standardizing all the variables. This means that each variable will have mean zero, and standard deviation one. One reason to standardize variables is to have comparable scales. When you perform a regression analysis, the value of the computed coefficients will depend on the measurement scale of the associated predictors. A $\beta$ coefficient will be different if the variable is measuredin grams or in kilos. To avoid favoring a certain coefficient, it is recommended to mean-center and standardize the variables.

The scaled data we get from the two steps is saved as a csv file, called `scaled-credit.csv`. This is the actual data we are going to use for the model building process.# Methods


## Ordinary Least Squares Regression (OLS)
We first apply a multiple linear regression analysis on the data set `scaled_credit.csv`. In order to find the relationship between `Balance` and those financial and demogrphic variables, including `Age`, `Limit` and `Education`, we set up a linear model, which looks like this: $$Sales \approx \beta_0 + \beta_1*Income+ \beta_2*Limit + \dots + \beta_11*EthnicityCaucasian$$ where $\beta_0$ is the intercept term and the $\beta_is$ describe how each financial or demographic variable affects the sales. In this case, those $\beta$ coefficients are the least squares estimate of the actual values, estimated by minimizing the sum of the residual squared errors. Specifically, we are minimizing $$RSS = \sum {i=1}^{n}\left (y _{i}-\beta_0- \sum{j=1}^{p}\beta_jx_{ij} \right )^{2}$$, and minimizing this value over the $\hat{\beta_i}s$ results in $$\hat{\beta} = (X^TX)^{-1}X^TY$$ where Y is a vector with all the y values. [References from Wikipedia](https://en.wikipedia.org/wiki/Residual_sum_of_squares).

Bascially, minimizing the RSS would be minimizing the error of the prediction. Accoding to the Gauss-Markov Theorem, they are the best linear unbiased estimators in this model. 

## Shrinkage Methods
### Ridge Regression


Ridge regression is a shrinkage method that constrain the coefficient estimates to help to reduce variance and thus get better estimation. In ridge regression, we are minimizing $$RSS + \lambda\sum_{j=1}^{p}\beta_j^{2}$$, comparing to the OLS one, you can see the ridge regression has a term $\lambda\sum_{j=1}^{p}\beta_j^{2}$, which is called _shrinkage penalty_, which has the effect of shrinking the estimates of coefficients towards zero. $\lambda$ in this term is called _tuning parameter_, which serves to control the relative impact on the regression coefficient estimates. When $\lambda = 0$, the penalty term will have no effect, and the ridge regression will produce the least squares estimates. However, when $\lambda$ approaches infnity, the impact of the shrinkage penalty will grow, and hte ridge regression coefficient estimates will approach zero. 

### The Lasso


The _lasso_ is also a shrinkage method and it is a relatively recent alternative to ridge regression that overcomes ridge's disadvantage, which we will explain specifically in the later section. Thus, the _lasso_ is quite similar to ridge regression. The lasso coefficients minimize the quantity: $$\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|$$. As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection. As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. 

Additionally, to define a best $\lambda$ for the minimization, we are going to use cross validation, which is model validation technique for assessing how the results of statistical analysis will generalize to an independent data set. In a prediction problem, a model is usually given a dataset of known data on which training data set is run , and a testing dataset against which the model is tested. The goal of cross validation is to define a dataset to "test" the model in the training phase, in order to limit problems like overfitting.

## Dimension Reduction Methods
### Principal Components Regression


Principal components regression (PCR) is a dimension reduction technique for regression, involves constructing the first $M$ principal components, $Z_1 + \dots + Z_{M*}$, and then using these components as the predictors in a linear regression model that is fit using least squares. The key idea is that often a small number of principal components suffice to explian most of the variability in the data, as well as the relationship with the response. This model is better than using OLS, because we can avoid overfitting in this model, and the model will not pick up any unnecessary variability. In PCR, the variance will increase as the number of variables in the model increases, while the bias will decrease. 

Furthermore, the number of principal components, $M$, is also chosen by cross-validation. It is also crucial to standardize the variables when running PCR, because the scale of each variable may effect variance and thus affect the fit of the model produced. 

### Partial Least Squares


Partial least squares (PLS), is a supervised alternative to PCR. Like PCR, PLS is a dimension reduction method, while unlike PCR, PLS makes uses of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response. Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.

We now describe how the first PLS direction is computed. After standardizing the $p$ predictors, PLS computes the first direction $Z_1$ by setting each $\phi_{j1}$ in  equal to the coefficient from the simple linear regression of $Y$ onto $X_j$. One can show that this coefficient is proportional to the correlation between $Y$ and $X_j$. Hence, in computing $$Z_1=\sum_{j=1}^p\phi_{j1}X_j$$, PLS places the highest weight on the variables that are most strongly related to the response.


# Analysis


This is the table of regression coefficients for all methods.
```{r results= 'asis', echo = FALSE, message=FALSE, warning=FALSE}
Variables <- c("Intercept", "Income","Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
OLS <- as.vector(ols_coefficients)
Ridge <- as.vector(ridge_coefficients)
Lasso <- as.vector(lasso_coefficients)
PCR <- as.vector(pc_coef_full)
PCR <- append(PCR, 0, 0)
PLSR <- as.vector(pls_coef_full)
PLSR <- append(PLSR, 0, 0)

five_matrix_coef <- data.frame(Variables, OLS, Ridge, Lasso, PCR, PLSR)
print(xtable(five_matrix_coef, caption = 'Regression Coefficients for 5 Regression Methods', digits = 3), comment = FALSE, type = 'latex')
```


## OLS Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
library(Matrix)
load("../data/output/ols-regression.RData")
print(xtable(ols_summary$coefficients, caption = 'OLS Coefficients',digits = c(0,5,5,5,5)), comment = FALSE,type = 'latex')
```

OLS is the benchmark for the comparison between different models. From the OLS regression results, we find that some coefficients have a very big p-value, thus they are not statistically significant. Therefore, Education, Gender, Marital Status and Ethnicity do not seem to have a relationship with Balance. Furthermore, we also find that some statistically significant regressors with small coefficients, which means that has very small economic effect on Balance. Thus, the main factors that may have relationship with Balance are Income, Limit and Rating.

## Ridge Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/ridge-regression.RData")
ridge_matrix_coef <- as.matrix(ridge_coefficients)
colnames(ridge_matrix_coef) <- "Estimates"
print(xtable(ridge_matrix_coef, caption = 'Ridge Coefficients',digits = c(0,5)), comment = FALSE,type = 'latex')
```

In this Ridge regression, we found that results in the smallest validation error is $\lambda$ = `r ridge_min_lambda`. This tuning parameter $\lambda$ is relatively small. Comparing ridge regression coefficients with OLS coefficients, we find that the estimation with ridge is very similar to that of OLS and has relatively smaller coefficients, expect the Rating coefficient, is much larger in ridge.

$$\includegraphics[width=250pt]{../images/ridge-cross-validation-errors.png}$$

## Lasso Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/lasso-regression.RData")
lasso_matrix_coef <- as.matrix(lasso_coefficients)
colnames(lasso_matrix_coef) <- "Estimates"
print(xtable(lasso_matrix_coef, caption = 'Lasso Coefficients', digits = c(0,5)), comment = FALSE,type = 'latex')
```

Lasso is an improved alternative to ridge, as it adds the incentive to render statistically insignificant estimate to 0 by performing both variable slection and only fitting the data to the variables that fit the MSE criteria.  Here, the $\lambda$ we find is $\lambda$ = `r lasso_min_lambda`. Moreover, six of the coefficients have been set to zero, the rest of the lasso coefficients also tend to be smaller than those from OLS. This is due to the added restriction based on ridge regression.

$$\includegraphics[width=250pt]{../images/lasso-cross-validation-errors.png}$$


## Principal Components Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/pc-regression.RData")
pcr_matrix_coef <- as.matrix(pc_coef_full)
dimnames(pcr_matrix_coef) <- list(rownames(pcr_matrix_coef, do.NULL = FALSE, prefix = "row"),colnames(pcr_matrix_coef, do.NULL = FALSE, prefix = "col"))
colnames(pcr_matrix_coef) <- "Estimates"
rownames(pcr_matrix_coef) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(pcr_matrix_coef, caption = 'PCR Coefficients',digits = c(0,5)), comment = FALSE,type = 'latex')
```


In this case, we find that the best M is `r lambda_min_pc`. Comparing coefficients between PCR and OLS, we find that they are almost same to each other. Additionally, the coefficients of PCR is very similar to ridge and PLSR regression.

$$\includegraphics[width=250pt]{../images/cv-pc-mse-plot.png}$$


## Partial Least Squares Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/pls-regression.RData")
pls_matrix_coef <- as.matrix(pls_coef_full)
dimnames(pls_matrix_coef) <- list(rownames(pls_matrix_coef, do.NULL = FALSE, prefix = "row"),colnames(pls_matrix_coef, do.NULL = FALSE, prefix = "col"))
colnames(pls_matrix_coef) <- "Estimates"
rownames(pls_matrix_coef) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(pls_matrix_coef, caption = 'PLS Coefficients',digits = c(0,5)), comment = FALSE,type = 'latex')
```

PLS is a supervised alternative to PCR. By comparing validation errors for different Ms, we find the best M is `r lambda_min_pls`. Comparing coefficients of PLS to OLS coefficients, we can see they are almost same to each other.

$$\includegraphics[width=250pt]{../images/cv-pls-mse-plot.png}$$

# Results

```{r results= 'asis', echo = FALSE}
MSE_five = as.matrix(c("ols" = ols_mse, "ridge" = ridge_mse, "lasso" = lasso_mse, "pcr" = pc_mse, "plsr" = pls_mse))
colnames(MSE_five) = "MSE"
print(xtable(MSE_five, caption = 'MSE of Five Regression Methods', digits = 5), comment = FALSE)
```

From the MSE table above, we find that all the MSE are very close to each other and the maximum difference between the MSE of ols and that of lasso. The regression with the smallest MSE is ols regression, where MSE = `r ols_mse`. As ols has the smallest MSE, then we can conlcude that ols is the best fitted model for the credit data set in this case. Aditionally, the one with the largest MSE is lasso regression, where MSE is euqual to `r lasso_mse`. this means tha lasso regression provides the worst fitted model for the credit dataset.This is a very interesting finding that extended approaches get worse model than ols regression. This is may because we do not have a very large number of variables or low ratio of number of observations to number of variables. Moreover, it is also possible that preditors ae highly correlated, thus lasso does not yeild good results in presense of high collinearity, results in the worst fit model.

Furthermore, we also include a trendline plot in which the official coefficients are compared.

```{r results= 'asis', echo = FALSE}
par(mfrow = c(1,1))
plot(five_matrix_coef[,2], xaxt = "n",xlab=" ", ylab = "Value", main = "Trend Lines of Coefficients for Five Regression Models", col = "black")
lines(five_matrix_coef[,2], col="black", lwd=2)
points(five_matrix_coef[,3],col= "red")
lines(five_matrix_coef[,3],col = "red")
points(five_matrix_coef[,4], col = "green")
lines(five_matrix_coef[,4],col = "green")
points(five_matrix_coef[,5], col = "yellow")
lines(five_matrix_coef[,5],col = "yellow")
points(five_matrix_coef[,6], col = "blue")
lines(five_matrix_coef[,6],col = "blue")
axis(1, at=1:12, labels=Variables, las=2,cex.axis=0.6)
legend(10,0.9,c('OLS','Ridge','Lasso','PCR','PLSR'), lty = c(1,1,1,1,1), lwd = c(2.5,2.5,2.5,2.5,2.5), col = c("black","red","green","yellow","blue"),merge = T)
abline(h = 0, lty = 3)
```

This trendline plot also allows us to compare how each type of regression models fit a model to the credit data set. We can see from the graph that the lines of PLSR, PCR and OLS are very close to each other, and even overlap. Lasso has the biggest variance with OLS generally. Ridge generally has smaller coefficients with OLS. Moreover, for variables like `Income`, it is almost same in each model, however, for varibles like `Rating` and `Limit`, these estimates vary widely in different regression methods.




# Conclusions

In conclusion, in this project, we fitted 5 types of regression models on the dataset `credit.csv` and we found that even though the MSE from all the regression models are quite similar, the lasso regression model generated the lowest MSE, at `r lasso_mse`. lasso regression also seems to have the least variance with OLS as shown from the trendline plot. This seems to indicate that lasso regression model is the best fitting model for predicting `balance` from the predictor variables in this dataset. In addition, the ridge regression model generated the highest MSE, indicating that it might be the worst fitting model.
