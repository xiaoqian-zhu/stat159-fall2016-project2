# Introduction


In this project we are going to follow the analysis from _Linear Model Selection and Regularization_(chapter 6) of the *An Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. [Link to the book](http://www-bcf.usc.edu/~gareth/ISL/). Applying each model on the data set _Credit_, [Link to the data set](http://www-bcf.usc.edu/~gareth/ISL/Credit.csv), we are going to predict the variable `Balance ` in terms of ten predictors such as `Income`, `Age`, `Education`, `Gender`, `Ethnicity`, etc and also describe the differences between these models.Addtionally, this project involves working collaboratively in teams of two members.

In previous homework, we used simple and multiple linear regression for data analysis, while in this project, we consider some approaches for improving the simple linear model, by replacing plain least squares fitting with some alternative fitting procedures. _So, why might we want to use another fitting procedure instead of least squares?_

The linear model assumes that the true relationship between the response and the predictors is approximately linear and then, the least squares estimates will have low bias. If the number of observations is larger than the number of variables, the least squares estimates tend to have low variace and hence will perform well on test observations. However, if number of observations is not larger than the number of variables, then there can be a lot of variabiblity in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. If the number of observations is smaller than the number of variables, then the method could not be used. This results in failures in _prediction accuracy_. 

Moreover, It is often the case that some or many of the
variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero.This results in failures in _Model Interpretability_.

Using alterantive fitting procedures can yield better prediction accuracy and model interpretability, by constraining or shrinking the estimated coefficients and performing feature selection or variable selection, and also reducing dimension. 

In this report, we are going to focus shrinkage methods and dimension reduction method. Ridge and Lasso regressions are shrinkage methods. PCR and PLSR are dimension reduction methods. Functions to fit models with ridge and lasso regressions are available in the package `"glmnet"`. Functions to fit models with PCR and PLSR are available in the package `"pls"`. We use the multiple linear regression model via Ordinary Least Squares(OLS) based on the data set `credit` as the benchmark to compare the other methods. 


