
# Analysis


This is the table of regression coefficients for all methods.
```{r results= 'asis', echo = FALSE}
Variables <- c("Intercept", "Income","Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
OLS <- as.vector(ols_coefficients)
Ridge <- as.vector(ridge_coefficients)
Lasso <- as.vector(lasso_coefficients)
PCR <- as.vector(pc_coef_full)
PCR <- append(PCR, 0, 0)
PLSR <- as.vector(pls_coef_full)
PLSR <- append(PLSR, 0, 0)

five_matrix_coef <- data.frame(Variables, OLS, Ridge, Lasso, PCR, PLSR)
print(xtable(five_matrix_coef, caption = 'Regression Coefficients for 5 Regression Methods', digits = 3), comment = FALSE, type = 'latex')
```


## OLS Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
library(Matrix)
load("../data/output/ols-regression.RData")
print(xtable(ols_summary$coefficients, caption = 'OLS Coefficients',digits = c(0,5,5,5,5)), comment = FALSE,type = 'latex')
```

OLS is the benchmark for the comparison between different models. From the OLS regression results, we find that some coefficients have a very big p-value, thus they are not statistically significant. Therefore, Education, Gender, Marital Status and Ethnicity do not seem to have a relationship with Balance. Furthermore, we also find that some statistically significant regressors with small coefficients, which means that has very small economic effect on Balance. Thus, the main factors that may have relationship with Balance are Income, Limit and Rating.

## Ridge Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/ridge-regression.RData")
ridge_matrix_coef <- as.matrix(ridge_coefficients)
colnames(ridge_matrix_coef) <- "Estimates"
print(xtable(ridge_matrix_coef, caption = 'Ridge Coefficients',digits = c(0,5)), comment = FALSE,type = 'latex')
```

In this Ridge regression, we found that results in the smallest validation error is $\lambda$ = `r ridge_min_lambda`. This tuning parameter $\lambda$ is relatively small. Comparing ridge regression coefficients with OLS coefficients, we find that the estimation with ridge is very similar to that of OLS and has relatively smaller coefficients, expect the Rating coefficient, is much larger in ridge.

$$\includegraphics[width=250pt]{../images/ridge-cross-validation-errors.png}$$

## Lasso Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/lasso-regression.RData")
lasso_matrix_coef <- as.matrix(lasso_coefficients)
colnames(lasso_matrix_coef) <- "Estimates"
print(xtable(lasso_matrix_coef, caption = 'Lasso Coefficients', digits = c(0,5)), comment = FALSE,type = 'latex')
```

Lasso is an improved alternative to ridge, as it adds the incentive to render statistically insignificant estimate to 0 by performing both variable slection and only fitting the data to the variables that fit the MSE criteria.  Here, the $\lambda$ we find is $\lambda$ = `r lasso_min_lambda`. Moreover, six of the coefficients have been set to zero, the rest of the lasso coefficients also tend to be smaller than those from OLS. This is due to the added restriction based on ridge regression.

$$\includegraphics[width=250pt]{../images/lasso-cross-validation-errors.png}$$


## Principal Components Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/pc-regression.RData")
pcr_matrix_coef <- as.matrix(pc_coef_full)
dimnames(pcr_matrix_coef) <- list(rownames(pcr_matrix_coef, do.NULL = FALSE, prefix = "row"),colnames(pcr_matrix_coef, do.NULL = FALSE, prefix = "col"))
colnames(pcr_matrix_coef) <- "Estimates"
rownames(pcr_matrix_coef) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(pcr_matrix_coef, caption = 'PCR Coefficients',digits = c(0,5)), comment = FALSE,type = 'latex')
```


In this case, we find that the best M is `r lambda_min_pc`. Comparing coefficients between PCR and OLS, we find that they are almost same to each other. Additionally, the coefficients of PCR is very similar to ridge and PLSR regression.

$$\includegraphics[width=250pt]{../images/cv-pc-mse-plot.png}$$


## Partial Least Squares Regression

```{r results= 'asis', echo = FALSE, error=FALSE}
load("../data/output/pls-regression.RData")
pls_matrix_coef <- as.matrix(pls_coef_full)
dimnames(pls_matrix_coef) <- list(rownames(pls_matrix_coef, do.NULL = FALSE, prefix = "row"),colnames(pls_matrix_coef, do.NULL = FALSE, prefix = "col"))
colnames(pls_matrix_coef) <- "Estimates"
rownames(pls_matrix_coef) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(pls_matrix_coef, caption = 'PLS Coefficients',digits = c(0,5)), comment = FALSE,type = 'latex')
```

PLS is a supervised alternative to PCR. By comparing validation errors for different Ms, we find the best M is `r lambda_min_pls`. Comparing coefficients of PLS to OLS coefficients, we can see they are almost same to each other.

$$\includegraphics[width=250pt]{../images/cv-pls-mse-plot.png}$$
